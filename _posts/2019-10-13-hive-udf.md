---
layout: post
title: "Integrating Hive UDF's in Python"
excerpt_separator: <!--excerpt-->
tags: [python, hive]
---

I need to do some pretty flexible things in my Hive queries, so flexible
that it's beyond the capability of Hive QL.
Writing a Hive UDF (user defined function) is an option.
However, all the online examples I can find require the UDF to be a standing-alone script,
placed at a known location in HDFS, and used via the `ADD FILE` clause that is understood by the Hive CLI.
Having to put the script in HDFS and use it on a machine that has the Hive CLI installed means interruption to my Python code flow, which I hate.
I want the Hive UDF to be seamlessly integrated into my Python code.
How can I do that?

## Preparations

Before describing my solution to UDF, I need to make two preparations:
the first is a Python client that easily interacts with Hive server, as long as UDF is not involved;
the second is a toy Hive table for testing.

First, the Python client is developed based on the package `impyla`.
Skipping all the details, the client is used like this to create and query a table `mytable`:

```python
hive = Hive()

sql = '''
CREATE TABLE mytable (
    a INT, 
    b STRING, 
    c STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE'''

hive.write(sql)

sql = 'SELECT a, b, c FROM mydb.mytable'
hive.read(sql)
rows = hive.fetchall()
```

This can execute any valid query, and fetching results following a `read` query,
without ever leaving the Python code flow.

Second, the toy table, `mydb.cars`, contains two columns: an integer `id`, and a JSON string `info_json`.
The following rows are inserted into the table:

```
1, '{"make": "honda", "price": 1000}'
2, '{"make": "ford", "price": 2000}'
3, '{"make": "ford"}'
4, '{"make": "tesla", "price": 3000}'
5, '{"make": "honda", "price": 2000}'
6, '{"make": "ford", "price": 4000}'
```

## The UDF syntax

A Hive UDF in Python is a regular Python script.
Suppose the script file is `myudf.py`.
The basic syntax for using the UDF is as follows:

```
ADD FILE hdfs:///tmp/myudf.py;

SELECT 
    TRANSFORM(id, info_json)
    USING 'myudf.py'
    AS (out STRING)
FROM mydb.cars
```

Here, the rows retrieved by `FROM (SELECT ...)` are processed by `myudf.py`, which prints out a single column, being called `out`. The input fields to `myudf.py` are specified by `TRANSFORM(...)`.

The `FROM` clause, as usual, can use a sub-query like this:

```
...
FROM (
    SELECT * FROM mydb.cars
) AS tmp
```

with all kinds of variations.

Let's make `myudf.py` very simple---it simply returns the second field:

```python
#!/usr/bin/env python
from __future__ import print_function
import sys

SEP = '\t'
NULL = '\\N'

def main():
    for line in sys.stdin:
        _, info_json = line.strip().split(SEP)
        print(info_json)


if __name__ == '__main__':
    main()
```

Here's how a UDF script works:

1. Input rows are fed to the script on `stdin`.
2. The script takes each input row and prints out something, which is taken as the output.
   If the print-out contains the field separator `'\t'`, then it is interpreted as multiple columns.
   In the example above, the print-out contains a single column, which is named `out`
   in the HiveQL statement that uses this UDF.
3. Because print-outs to `stdout` are taken as the result, there is no way to enforce that
   each input line has to correspond to one output line.
   If the script prints multiple lines for an input line, they are simply all result lines.
   If the script does not print anything while processing a certain input line,
   that amounts to filtering out that input row.

I came to the following critical realization, which is not emphasized in any of the online tutorials I found:

> The `xyz` specified in `TRANSFORM(...) USING 'xyz'` must be a program that can run on the Hive server.
> If it requires some program, an environment variable, or any other dependencies, these should all be in place, because the program `xyz` must execute on that server machine.
> This also implies that the program must be on the system `PATH` so that it can be found.
> The program should take input and print out output as described above.
>
> And this is all that needs to be satisfied.
> It does not matter what language the script is written in---e.g. it's perfectly fine if it's a binary executable produced by some `C++` code---as long as it just runs.
> The `ADD FILE` statement is not essential---again, as long as it runs.

Let's verify it. Let's make a UDF that is simply `echo xyz`.
This UDF ignores any input, and just prints out a single line of output, `xyz`.
I logged into a machine that has the `hive` CLI, and did this:

```
$ hive
hive> use mydb;
OK
Time taken: 0.014 seconds
hive> SELECT TRANSFORM(id, info_json) USING 'echo xyz' AS (out STRING) FROM cars;
Total jobs = 1
Launching Job 1 out of 1
... ...
Total MapReduce CPU Time Spent: 1 seconds 940 msec
OK
xyz
Time taken: 47.852 seconds, Fetched: 1 row(s)
hive>
```

As expected, the 'program' `echo xyz` was run and printed a single row

```
xyz
```

This points in the direction of effort: if I can somehow include the entire UDF in the string '...' after `USING`, then I don't need to deal with creating a script, putting it in HDFS, and so on. It should just work!

Let the journey begin.

## Strategy

My goal is to write "in-line" Hive UDF in Python and send this code over to Hive server as part of the HiveQL statement, hence avoiding any script files. The way to run Python code without a script file is

```
$ python -c "python_code"
```

therefore the idea is to write something like

```
TRANSFORM(...)
USING 'python -c "python_udf_code"'
```

On the Hive server that I work with,
the Python version is 2.7, and the Hive version is 0.13. Both old versions will come haunt me in just a little bit. Keep reading.

First, let's make up a naive UDF to see it *can* work.
The most naive UDF I can think of contains one statement, `print 123`:

```
hive> SELECT TRANSFORM(id, info_json) USING 'python -c "print 123"' AS (out STRING) FROM cars;
Total jobs = 1
Launching Job 1 out of 1
... ...
Total MapReduce CPU Time Spent: 1 seconds 900 msec
OK
123
Time taken: 303.91 seconds, Fetched: 1 row(s)
hive>
```

Although painfully slow, the `123` in the print-out is really encouraging!


## Trouble begins

What if I want to print a string instead of the number `123`? The quotes around the string need to be escaped, which shouldn't be much of an issue. Let's try it:

```
hive> SELECT TRANSFORM(id, info_json) USING 'python -c "print \'abc\'"' AS (out STRING) FROM cars;
...
Ended Job = job_1558314961036_2285 with errors
Error during job, obtaining debugging information...
...
Cased by: org.apache.hadoop.hive.ql.metadata.HiveException: [Error 20003]: An error occurred when trying to close the Operator running your custom script.
...
FAILED: Execution Error, returning code 20003 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. An error occurred when trying to close the Operator running your custom script.
...
```

Ooops! The straightforward quote escaping does not work.
Let's skip this for now.
The above is running the Hive CLI on a "gateway" box.
Let's switch to my laptop and experiment with the Hive client code mentioned above.

```python
>>> hive = Hive()
>>> sql = '''
>>>     SELECT
>>>         TRANSFORM(
>>>             id,
>>>             info_json
>>>         )
>>>         USING 'python -c "print 123"'
>>>         AS (out STRING)
>>>     FROM mydb.cars
>>> '''
>>>
>>> hive.read(sql)
>>> hive.fetchall()
[('123',)]
```

It worked. Beautiful.

Now let's try printing a string.


```python
>>> sql = '''
>>>     SELECT
>>>         TRANSFORM(
>>>             id,
>>>             info_json
>>>         )
>>>         USING 'python -c "print \'abc\'"'
>>>         AS (out STRING)
>>>     FROM mydb.cars
>>> '''
>>>
>>> hive.read(sql)
...
impala.error.HiveServer2Error: Failed after retrying 3 times
```

The error message is not the most useful. For two or three hours, I tried every way I could think of to escape the quotation marks. No luck. This is not working. Printing strings has to be supported. The need is simply inevitable, of course.

The trouble is this: 

- the program after `USING` has to be quoted, like `'python ...'`;
- the Python script in there has to be quoted, like `'python -c "..."'`;
- then in the script, quotation mark has to be escaped.

But it does not work.

After a few more hours combing the internet, I found the reason: Hive had a bug about "string literal escaping". The bug has been fixed in version 2.0.0, but was not back-ported.
Remember my Hive version is 0.13? It came out in April 2014.
On the other hand, Hive 2.0.0 came out in Feb 2016. What can I do?

Even if escaping worked, it probably is not enough---my Python UDF may be a sophisticated piece of code. It's hard to prescribe what can be written in there. To pass that code as a long string in this fragile HiveQL statement is likely going to fail any moment. So the challenge is: how can I pass a block of arbitrary Python code (the UDF) through HiveQL, where quotation escaping does not work?

## Passing Python code through HiveQL

The solution is to transform the Python code into something that would safe too pass through HiveQL, and back-transform it in the Python script before executing it. This transformed format should be a simple blob of characters without all kinds of peculiarities. Roughly, I need something like this:

```
USING 'python -c "exec(back_transform(blob))"'
```

What should 'blob' be? Is it a string that needs to be quoted? That would have the same problem...

Regarding the "transform", occurred to me first was "compress". But quickly that gave way to `Base64` encoding/decoding. I had not used it before, and knew (and know) very little about it. Somehow it looked right to me. According to Wikipedia,

> Base64 is a group of binary-to-text encoding schemes that represent binary data in an ASCII string format... Base64 is designed to carry data stored in binary formats across channels that only reliably support text content. Base64 is particularly prevalent on the World Wide Web...

Remember I'm using Python 3.6, whereas the Hive server is using Python 2.7. Suppose the UDF code is `s`, I first encode it to bytes, then Base64-encode it to a string by

```python
base64.urlsafe_b64enode(s.encode())
```

The encoded string may contain `'a-z'`, `'A-Z'`, `'0-9'`, and `'-'`, `'_'`. Hive will not complain about it. Let's take the simple UDF above, repeated below:


```bash
$ python3
Python 3.6.7 (default, Oct 22 2018, 11:32:17) 
[GCC 8.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> s = '''\
... from __future__ import print_function
... import sys
... 
... SEP = '\t'
... NULL = '\\N'
... 
... def main():
...     for line in sys.stdin:
...         _, info_json = line.strip().split(SEP)
...         print(info_json)
... 
... 
... if __name__ == '__main__':
...     main()
... '''
>>> import base64
>>> encoded = base64.urlsafe_b64encode(s.encode())
>>> encoded
b'ZnJvbSBfX2Z1dHVyZV9fIGltcG9ydCBwcmludF9mdW5jdGlvbgppbXBvcnQgc3lzCgpTRVAgPSAnCScKTlVMTCA9ICdcTicKCmRlZiBtYWluKCk6CiAgICBmb3IgbGluZSBpbiBzeXMuc3RkaW46CiAgICAgICAgXywgaW5mb19qc29uID0gbGluZS5zdHJpcCgpLnNwbGl0KFNFUCkKICAgICAgICBwcmludChpbmZvX2pzb24pCgoKaWYgX19uYW1lX18gPT0gJ19fbWFpbl9fJzoKICAgIG1haW4oKQo='
>>> 
```

To be sure, we can get back the code (please note the switch between Python versions):

```bash
$ python
Python 2.7.15rc1 (default, Nov 12 2018, 14:31:15) 
[GCC 7.3.0] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> encoded = 'ZnJvbSBfX2Z1dHVyZV9fIGltcG9ydCBwcmludF9mdW5jdGlvbgppbXBvcnQgc3lzCgpTRVAgPSAnCScKTlVMTCA9ICdcTicKCmRlZiBtYWluKCk6CiAgICBmb3IgbGluZSBpbiBzeXMuc3RkaW46CiAgICAgICAgXywgaW5mb19qc29uID0gbGluZS5zdHJpcCgpLnNwbGl0KFNFUCkKICAgICAgICBwcmludChpbmZvX2pzb24pCgoKaWYgX19uYW1lX18gPT0gJ19fbWFpbl9fJzoKICAgIG1haW4oKQo='
>>> type(encoded)
<type 'str'>
>>> import base64
>>> decoded = base64.urlsafe_b64decode(encoded)
>>> print(decoded)
from __future__ import print_function
import sys

SEP = '	'
NULL = '\N'

def main():
    for line in sys.stdin:
        _, info_json = line.strip().split(SEP)
        print(info_json)


if __name__ == '__main__':
    main()

>>> 
```

This looks really promising. At this point I imagined my UDF would look like this:

```python
import base64
code = 'ZnJvbSBfX2Z1dHVyZV9fIGltcG9ydCBwcmludF9mdW5jdGlvbgppbXBvcnQgc3lzCgpTRVAgPSAnCScKTlVMTCA9ICdcTicKCmRlZiBtYWluKCk6CiAgICBmb3IgbGluZSBpbiBzeXMuc3RkaW46CiAgICAgICAgXywgaW5mb19qc29uID0gbGluZS5zdHJpcCgpLnNwbGl0KFNFUCkKICAgICAgICBwcmludChpbmZvX2pzb24pCgoKaWYgX19uYW1lX18gPT0gJ19fbWFpbl9fJzoKICAgIG1haW4oKQo='
exec(base64.urlsafe_b64decode(code))
```

and I'd write the Hive snippet this way

```
USING 'python -c "import base64; code = ...; exec(...)"'
```

But wait, the `code` variable is a string. The string value needs quotation marks. Even if `code` takes its value from a variable, that variable has to take its string value earlier. At some point, the string has to get into the Python code, and without quotation marks, no string value can be represented to begin with. And my adorable five-year-behind-its-time Hive server can not handle those quotation marks... ...

I searched whether there is a way in Python to create string values without quotation marks---hoping something like `quote(abc)` is equivalent to `'abc'`. I did not find it. (I think R has something like that.)

As I'm writing this I think I must be in a pretty creative mode that day. I came up with a solution to this problem without too much difficulty.

How can I get a string value into Python code without the help of quotation marks? When the code takes command-line arguments, the value provided is a string that does not need to be quoted, the value taken is a string, and quotation marks do not appear on the receiving side, either.
The code goes like this:

```python
encoded = base64.b64encode(s.encode('utf-8'))

script = 'import sys, base64; code = sys.argv[1]; exec(base64.b64decode(bytes(code)));'

code = f'python -c "{script}" {str(encoded)[2:-1]}'
```

In the last line above, when I feed the encoded string (actually bytes in Python 3) to the script, I need to get rid of the quotation marks. To this end, I first turn the bytes to string,

```
>>> str(encoded)
"b'ZnJvbSBfX2Z1dHVyZV9fIGltcG9ydCBwcmludF9mdW5jdGlvbgppbXBvcnQgc3lzCgpTRVAgPSAnCScKTlVMTCA9ICdcTicKCmRlZiBtYWluKCk6CiAgICBmb3IgbGluZSBpbiBzeXMuc3RkaW46CiAgICAgICAgXywgaW5mb19qc29uID0gbGluZS5zdHJpcCgpLnNwbGl0KFNFUCkKICAgICAgICBwcmludChpbmZvX2pzb24pCgoKaWYgX19uYW1lX18gPT0gJ19fbWFpbl9fJzoKICAgIG1haW4oKQo='"
```

This string has three extra characters I don't want: `b'` at the beginning and `'` at the end, so I use the `str(encoded)[2:-1]` substring to get rid of them. In the end, the string
`code` is actually this:

```python
'python -c "import sys, base64; code = sys.argv[1]; exec(base64.b64decode(bytes(code)));" ZnJvbSBfX2Z1dHVyZV9fIGltcG9ydCBwcmludF9mdW5jdGlvbgppbXBvcnQgc3lzCgpTRVAgPSAnCScKTlVMTCA9ICdcTicKCmRlZiBtYWluKCk6CiAgICBmb3IgbGluZSBpbiBzeXMuc3RkaW46CiAgICAgICAgXywgaW5mb19qc29uID0gbGluZS5zdHJpcCgpLnNwbGl0KFNFUCkKICAgICAgICBwcmludChpbmZvX2pzb24pCgoKaWYgX19uYW1lX18gPT0gJ19fbWFpbl9fJzoKICAgIG1haW4oKQo='
```

This whole string is in single quotes; the script after `-c` is in double quotes; the sole command-line argument (i.e. the long encoding of the actual UDF) is not quoted.


(First half drafted in May 17, 2019. Finished on October 12, 2019.)