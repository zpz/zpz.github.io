---
layout: post
title: "Biglist: a Single-Machine, Out-of-Memory Long Sequence"
excerpt_separator: <!--excerpt-->
tags: [python]
---

When dealing with large amounts of data, a rather common situation is that
the amount of data is manageable on a single machine,
but can be unwieldy to be loaded at once into memory, for example,
when the data size (on disk) is above 10GB and growing.<!--excerpt>

We need a solution to persist these data on disk, and only load parts of it
into memory as needed. In addition, we should not have to store all the data
in a single file, which will become unscalable sooner or later.
Instead, we should manage a number of files in a way that is transparent
to the user. The number of files will grow with the amount of data,
limited only by the disk size.

In this post, we'll tackle a specific yet very common case, which is, the data consist of a long sequence of "records".
A natural idea is to build a tool to operate on this data,
and the operations are better as close to a regular `list` as possible.
We are going to create a Python utility called `biglist` for this.

At the start, these are the design requirements:

1. It takes a directory as the storage location.
   What happens within the directory is pretty transparent to the user.
2. It supports `append` and `extend`, just like the built-in `list`.
3. It supports element access by index, slice, and iteration.
4. It does not need to support mutation of existing elements, that is, it's append-only.
5. The type of elements are very general, as long as they can be pickled,
   because we'll store them using `pickle`.

Let's get started.

## Laying the groundwork

On a very high level, on-disk we'll have a small info file for meta data---such as number of files---and a data store for a series of data files.
We are going to name them by `0.pickle`, `1.pickle`, etc.
In-memory we're going to have a buffer for reading and another buffer for appending.
The size of both buffers is up to the size of a single file on disk,
which is indicated by the count of elements; we'll have a parameter called `batch_size` to control it.

The append-buffer is always "moving forward": elements are appended to it
until the count reaches `batch_size`; at that point the buffer's content will saved in a file, and the buffer will restart empty, waiting for new elements.

Independent of `append`, we may want to access any element of the list at any time. Depending on the index, we will load the relevant file into the read-buffer to provision the element.

First, we create a few helper functions:

```python
import json
import os, os.path
import pickle
from typing import Any


def json_load(path: str, *path_elements) -> Any:
    with open(os.path.join(path, *path_elements), 'r') as f:
        return json.load(f)


def pickle_load(path: str, *path_elements) -> Any:
    with open(os.path.join(path, *path_elements), 'rb') as f:
        return pickle.load(f)


def prepare_path(path: str, *path_elements):
    ff = os.path.join(path, *path_elements)
    dirname = os.path.dirname(os.path.abspath(ff))
    if not os.path.isdir(dirname):
        os.makedirs(dirname)
    return ff


def json_dump(x: Any, path: str, *path_elements) -> None:
    ff = prepare_path(path, *path_elements)
    with open(ff, 'w') as f:
        json.dump(x, f)


def pickle_dump(x: Any, path: str, *path_elements) -> None:
    ff = prepare_path(path, *path_elements)
    with open(ff, 'wb') as f:
        pickle.dump(x, f, protocol=pickle.HIGHEST_PROTOCOL)

```

Our main class starts like this:

```python
import shutil
import tempfile
from typing import Iterable


class Biglist:
    def __init__(
            self,
            path: str = None,
            batch_size: int = None,
            ):
        self.path = path
        self.file_lengths = []
        self.cum_file_lengths = [0]

        if not self.path:
            self.path = tempfile.mkdtemp()

        assert self.path.startswith('/')

        if os.path.isdir(self.path):
            z = os.listdir(self.path)
            if z:  
                # directory is not empty
                if not os.path.isfile(self.info_file) or not os.path.isdir(self.data_dir):
                    raise RuntimeError(f"path '{self._path}' is not empty "
                            f"but is not a valid {self.__class__.__name__} folder")
                info = json_load(self.info_file)

                self.file_lengths = info['file_lengths']
                batch_size = info['batch_size']
                for n in self.file_lengths:
                    self.cum_file_lengths.append(self.cum_file_lengths[-1] + n)
            else: 
                # directory is empty
                os.makedirs(self.data_dir)
        else:
            # directory does not exist
            os.makedirs(self.data_dir)

        if batch_size is None:
            batch_size = 10000
        else:
            assert batch_size > 0
        self.batch_size = batch_size
        self._read_buffer = None
        self._read_buffer_file_idx = None
        # `self._read_buffer` contains the content of the file 
        # indicated by `self._read_buffer_file_idx`.
        self._append_buffer = None

    @property
    def info_file(self) -> str:
        return os.path.join(self.path, 'info.json')

    @property
    def data_dir(self) -> str:
        return os.path.join(self.path, 'store')

    def data_file(self, file_idx: int) -> str:
        return os.path.join(self.data_dir, str(file_idx) + '.pickle_z')

    def __len__(self) -> int:
        n = self.cum_file_lengths[-1]
        if self._append_buffer:
            return n + len(self._append_buffer)
        return n

    def __bool__(self) -> bool:
        return len(self) > 0
```

Note that there are two situations while initializing a `Biglist` object.
In one situation, the specified location is nonexistent or empty--in this case,
a new Biglist is being created.
In another situation, the specified location is the the warehouse for a previously created Biglist--in this case, meta info gets read in, and the new `Biglist` object
is ready to read existing data, in addition to append new data.

## Writing data to files

An element is added to the end of the list by the method `append`.
The element is first appended to the in-memory list `self._append_buffer`.
Once the length of the buffer reaches `self.batch_size`,
as the last stage in `append`, the method `flush` is called to write
the content of the buffer to a file.
As such, the length of `self._append_buffer` will never exceed `self.batch_size`.
However, it can be anything below the capacity,
because the user is allowed to call `flush` anytime.
In the meantime, we also update the meta info file.

```python
class Biglist:

    def flush(self) -> None:
        if not self._append_buffer:
            return

        buffer_len = len(self._append_buffer)
        pickle_dump(self._append_buffer, self.data_file(len(self.file_lengths)))
        self.file_lengths.append(buffer_len)
        self.cum_file_lengths.append(self.cum_file_lengths[-1] + buffer_len)
        json_dump(
            {'file_lengths': self.file_lengths,
             'batch_size': self.batch_size,
             },
            self.info_file)

        self._append_buffer = [] if buffer_len == self.batch_size else None
```

The last line of the method sets the append-buffer to an empty list or `None`,
depending on whether the append-buffer was at capacity.
As a result, if the append-buffer is an empty list, it must have been
set by `flush` just after writing a full-length append-buffer.
In other words, the currently final data file has "full length",
meaning it contains as many as `self.batch_size` elements.

On the other hand, if the append-buffer is `None`,
it either has been set in `flush` after writing a partial-length data file,
or has not been touched since `__init__`, which sets it to `None`.
In the second case, the currently final data file on disk may also be partial.
Think when we initialize a `Biglist` object to read an existing Biglist
that has a partial-length final data file.

These two empty values of the append-buffer are useful indicators,
as we'll see shortly in `append`.

When we are done with appending elements to the list,
it's very likely that the append-buffer does not happen to be at capacity,
in which case the partial append-buffer has not been persisted.
We could ask the user to call `flush`.
As a safeguard, we also add a call to `flush`` when the object goes out of scope.

```python
class Biglist:

    def __del__(self):
        self.flush()
```

With this foundation in place, we are now ready to add elements to the list.

## Appending elements

If the append-buffer has some elements but has not reached capacity,
`append` simply appends to the buffer and calls `flush` as needed.
Simple logic.

If the append-buffer is an empty list, as has been explained above,
it's guaranteed that the currently final data file has "full length".
It's in good shape to start filling a new append-buffer,
hence `append` also simply appends to the append-buffer and calls `flush` as needed.

More thinking is needed if the append-buffer is `None`.
This happens either a previous call to `flush` has written a partial data file,
or the buffer has not need touch since being created with value `None` in `__init__`.
In the second case, the currently final data file, if any, may also be partial.

It is not essential to make all data files (except the final one) contain the same number
of elements. However, this regularity is nice to have.
Moreover, if we provide this guarantee, then some index calculations (when reading)
could be simplified.

We choose to provide this guarantee by the method called `_init_append_buffer`.
With this, `append` and `extend` are easy.

```python
class Biglist:

    def _init_append_buffer(self) -> None:
        if self.file_lengths and self.file_lengths[-1] < self.batch_size:
            self._append_buffer = pickle_load(
                self.data_file(len(self.file_lengths) - 1))

            if self._read_buffer_file_idx == len(self.file_lengths) - 1:
                self._read_buffer_file_idx = None
                self._read_buffer = None

            # Note:
            # do not delete the last file.
            # `flush` will overwrite this file if it exists.
            # If the current object is created only to read existing data,
            # then the user will not call `flush` after use.
            # By keeping the original file, data is not lost.
            self.file_lengths.pop()
            self.cum_file_lengths.pop()
        else:
            self._append_buffer = []

    def append(self, x) -> None:
        if self._append_buffer is None:
            self._init_append_buffer()

        self._append_buffer.append(x)
        if len(self._append_buffer) >= self.batch_size:
            self.flush()
            # Note that `flush` resets `self._append_buffer`.

    def extend(self, x: Iterable) -> None:
        for v in x:
            self.append(v)
```

In `_init_append_buffer`, if a partial data file is loaded into the append-buffer,
we check whether the read-buffer contains the content of the partial data file.
This could happen if we have accessed elements in the final file when the append-buffer
was still `None`.
Since the in-memory file list includes the "full-length" files only,
we need to null the read-buffer in this situation.


## Accessing a random element

Our `Biglist` implements the
["Sequence Protocol"](https://docs.python.org/3/c-api/sequence.html),
which, besides the special methods `__len__`, requires the method
`__getitem__` for accessing arbitrary element by index.
The main logic in this task involves determining which data file contains
the requested element. This is done in the method `_get_file_idx_for_item`,
listed below.

```python
class Biglist:

    def _load_file_to_buffer(self, idx: int):
        self._read_buffer = pickle_load(self.data_file(idx))
        self._read_buffer_file_idx = idx

    def _get_file_idx_for_item(self, idx: int) -> int:
        if idx >= self.cum_file_lengths[-1]:
            return len(self.file_lengths)
            # This suggests the requested element at index `idx`
            # resides in `self._append_buffer`.
        if self._read_buffer_file_idx is None:
            for k, n in enumerate(self.cum_file_lengths):
                if idx < n:
                    return k-1
        elif idx < self.cum_file_lengths[self._read_buffer_file_idx]:
            for k in range(self._read_buffer_file_idx - 1, -1, -1):
                if idx >= self.cum_file_lengths[k]:
                    return k
        elif idx >= self.cum_file_lengths[self._read_buffer_file_idx + 1]:
            for k in range(self._read_buffer_file_idx + 2, len(self.cum_file_lengths)):
                if idx < self.cum_file_lengths[k]:
                    return k - 1
        else:
            return self._read_buffer_file_idx

    def __getitem__(self, idx: int):
        '''
        Element access by single index; negative index works as expected.
        '''
        if not isinstance(idx, int):
            raise TypeError('A single integer index is expected. To use slice, use `view`.')

        idx = range(len(self))[idx]
        file_idx = self._get_file_idx_for_item(idx)

        if file_idx >= len(self.file_lengths):
            elem = self._append_buffer[idx - self.cum_file_lengths[-1]]
        else:
            if file_idx != self._read_buffer_file_idx:
                self._load_file_to_buffer(file_idx)

            n1 = self.cum_file_lengths[file_idx]
            n2 = self.cum_file_lengths[file_idx + 1]
            assert n1 <= idx < n2
            elem = self._read_buffer[idx - n1]

        return elem
```

A couple details need some explanation.
First, the whole Biglist is represented by the in-memory file list
plus the append-buffer (refer to the method `__len__` above).
The requested element may fall in the append-buffer.

Second, when the current read-buffer contains the content of a data file,
and the requested element is not in that file, we need to find out which file contains it.
This is a simple search based on the element counts of each data file.
Instead of a linear search from the first file to the last file,
we search backward from the current file (i.e. the file corresponding to
the current read-buffer) to the first file, or forward from the current file to the last file,
based on whether the requested element lies before or after the current file.
This approach is because, presumably, element access tends to be in sequential order
in most use cases.

## Iteration

Iteration can be trivially implemented using `__len__` and `__getitem__`.
However, `__getitem__` contains some logic to jump to any arbitrary index,
which is unnecessary overhead in a sequential walk-through.
The solution below walks through all the data files one by one,
followed by a walk-through of the append-buffer.

```python
class Biglist:

    def __iter__(self):
        for file_idx in range(len(self.file_lengths)):
            if file_idx == self._read_buffer_file_idx:
                buffer = self._read_buffer
            else:
                buffer = load_pickle(self.data_file(file_idx))
            yield from buffer

        if self._append_buffer is not None:
            yield from self._append_buffer
```

## Slicing

By convention,
slicing returns a new object of the same type as the original object.
Let's verify by a couple examples:

```
>>> x = range(13)
>>> y = x[-5:]
>>> type(y)
<class 'range'>
>>> type(x)
<class 'range'>
>>> 
>>> xx = list(x)
>>> yy = xx[2:7]
>>> type(yy)
<class 'list'>
>>> type(xx)
<class 'list'>
>>> 
```



```python

def slice_to_range(idx: slice, n: int) -> range:
    '''
    This functions takes a `slice`, combined with the length of the sequence,
    to determine an explicit value for each of `start`, `stop`, and `step`,
    and returns a `range` object.

    `n`: length of sequence
    '''
    if n < 1:
        return range(0)

    start, stop, step = idx.start, idx.stop, idx.step
    if step is None:
        step = 1

    if step == 0:
        raise ValueError('slice step cannot be zero')
    elif step > 0:
        if start is None:
            start = 0
        elif start < 0:
            start = max(0, n + start)
        if stop is None:
            stop = n
        elif stop < 0:
            stop = n + stop
    else:
        if start is None:
            start = n - 1
        elif start < 0:
            start = n + start
        if stop is None:
            stop = -1
        elif stop < 0:
            stop = max(-1, n + stop)

    return range(start, stop, step)


def regulate_range(idx: range, n: int) -> range:
    '''
    This functions takes a `range` (such as one returned by `slice_to_range`),
    combined with the length of the sequence,
    to refine the values of `start`, `stop`, and `step`,
    and returns a new `range` object.

    `n`: length of sequence
    '''
    start, stop, step = idx.start, idx.stop, idx.step
    if step > 0:
        if start >= n:
            start, stop, step = 0, 0, 1
        else:
            start = max(start, 0)
            stop = min(stop, n)
            if stop <= start:
                start, stop, step = 0, 0, 1
    else:
        if start < 0:
            start, stop, step = 0, 0, 1
        else:
            start = min(start, n -1)
            stop = max(-1, stop)
            if stop >= start:
                start, stop, step = 0, 0, 1

    return range(start, stop, step)


def view_slice_range(idx_on_view: range, view_range: range):
    len_ = len(view_range)
    range_ = regulate_range(slice_to_range(idx_on_view, len_), len_)
    if len(range_) == 0:
        return range_

    # The slice `idx` is on the 'view' of some 'source sequence'.
    # Need to convert it to a slice on the source sequence.

    start, stop, step = range_.start, range_.stop, range_.step
    start = view_range[start]
    step = view_range.step * step

    if stop >= 0:
        stop = view_range.start + view_range.step * stop
    else:
        assert stop == -1 and range_.step < 0
        if view_range.step > 0:
            stop = view_range.start - 1
        else:
            stop = view_range.start + 1

    return range(start, stop, step)
```

```python

    def view(self) -> 'BiglistView':
        # During the use of this view, the underlying Biglist should not change.
        # Multiple frozenview's may be used to view
        # different parts of the underlying Biglist--they open
        # and read files independent of other frozenview's.
        assert not self._append_buffer
        return BiglistView(self.path, self.__class__)
```

```python

class BiglistView:
    def __init__(self, path: str, biglist_cls: Type[Biglist] = None, range_: range = None):
        '''
        An object of `ListView` is created by `Biglist` or `BiglistView`.
        User should not attempt to create an object of this class directly.
        '''
        if biglist_cls is None:
            biglist_cls = Biglist
        assert path
        list_ = biglist_cls(path)
        if range_ is None:
            len_ = len(list_)
            range_ = range(len_)
        else:
            n = len(list_)
            range_ = regulate_range(range_, n)
            len_ = len(range_)
        self._biglist_cls = biglist_cls
        self._path = path
        self._list = list_
        self._range = range_

    def __len__(self) -> int:
        return len(self._range)

    def __bool__(self) -> bool:
        return len(self) > 0

    def __getitem__(self, idx: Union[int, slice]):
        '''
        Element access by a single index or by slice.
        Negative index and standard slice syntax both work as expected.

        Sliced access returns a new `BiglistView` object.
        '''
        if isinstance(idx, int):
            return self._list[self._range[idx]]

        if not isinstance(idx, slice):
            raise TypeError(f"an integer or slice is expected")

        return self.__class__(
            path=self._path,
            biglist_cls=self._biglist_cls,
            range_=view_slice_range(idx, self._range)
            )

    def _fileview_idx(self):
        if self._range.step != 1:
            return None
        try:
            idx = self._list.cum_file_lengths.index(self._range.start)
        except ValueError:
            return None
        if idx >= len(self._list.cum_file_lengths) - 1:
            return None
        if self._range.stop != self._list.cum_file_lengths[idx + 1]:
            return None
        return idx

    def __iter__(self):
        file_idx = self._fileview_idx()
        if file_idx is None:
            for idx in self._range:
                yield self._list[idx]
        else:
            yield from self._list.iterfile(file_idx)
```

## Supporting parallel processing

```python
class Biglist:

    def iterfile(self, file_idx):
        assert 0 <= file_idx < len(self.file_lengths)
        if file_idx == self._read_buffer_file_idx:
            buffer = self._read_buffer
        else:
            buffer = pickle_z_load(self.data_file(file_idx))
        yield from self._iter_buffer(buffer)

    def fileview(self, file_idx: int) -> 'BiglistView':
        assert not self._append_buffer
        assert 0 <= file_idx < len(self.file_lengths)
        return BiglistView(
            self.path, self.__class__,
            range(self.cum_file_lengths[file_idx], self.cum_file_lengths[file_idx+1]),
            )

    def fileviews(self) -> List['BiglistView']:
        return [
            self.fileview(i)
            for i in range(len(self.file_lengths))
        ]
```
